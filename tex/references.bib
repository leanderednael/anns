@article{byrd1995limited,
  title     = {A limited memory algorithm for bound constrained optimization},
  author    = {Byrd, Richard H and Lu, Peihuang and Nocedal, Jorge and Zhu, Ciyou},
  journal   = {SIAM Journal on Scientific Computing},
  volume    = {16},
  number    = {5},
  pages     = {1190--1208},
  year      = {1995},
  publisher = {SIAM}
}
@book{deisenroth2020mathematicsforml,
  title     = {Mathematics for Machine Learning},
  author    = {Marc Peter Deisenroth and A. Aldo Faisal and Cheng Soon Ong},
  year      = {2020},
  publisher = {Cambridge University Press},
  address   = {Cambridge},
  isbn      = {9781108455145},
  url       = {https://mml-book.github.io/}
}
@article{demircigil2017mhn,
  author  = {Demircigil, Mete and Heusel, Judith and L{\"o}we, Matthias and Upgang, Sebastian and Vermet, Franck},
  title   = {On a Model of Associative Memory with Huge Storage Capacity},
  journal = {Journal of Statistical Physics},
  volume  = {168},
  number  = {2},
  pages   = {288--299},
  year    = {2017},
  doi     = {10.1007/s10955-017-1806-y},
  url     = {https://link.springer.com/article/10.1007/s10955-017-1806-y}
}
@inproceedings{dosovitskiy2020vit,
  title     = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author    = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2021},
  note      = {arXiv preprint arXiv:2010.11929},
  url       = {https://arxiv.org/abs/2010.11929}
}
@inproceedings{duchi2011adaptive,
  title     = {Adaptive subgradient methods for online learning and stochastic optimization},
  author    = {Duchi, John and Hazan, Elad and Singer, Yoram},
  booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems (NeurIPS)},
  pages     = {257-265},
  year      = {2011},
  publisher = {Curran Associates Inc.}
}
@misc{dumoulin2018guideconvolutionarithmeticdeep,
  title         = {A guide to convolution arithmetic for deep learning},
  author        = {Vincent Dumoulin and Francesco Visin},
  year          = {2018},
  eprint        = {1603.07285},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML},
  url           = {https://arxiv.org/abs/1603.07285}
}
@book{gilhooly2014cognitive,
  title     = {Cognitive Psychology (1st ed.)},
  author    = {Gilhooly, K.J. and Gilhooly, K. and Lyddy, F. and Pollick, F.},
  isbn      = {9780077122669},
  series    = {UK Higher Education Psychology},
  year      = {2014},
  publisher = {McGraw-Hill}
}
@book{Goodfellowetal2016,
  title     = {Deep Learning},
  author    = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
  publisher = {MIT Press},
  year      = {2016},
  url       = {https://www.deeplearningbook.org/},
  isbn      = {9780262035613}
}
@misc{hinton2012rmsprop,
  author       = {Geoffrey Hinton and Nitish Srivastava and Kevin Swersky},
  title        = {Lecture 6e: {RMS}prop - {D}ivide the gradient by a running average of its recent magnitude},
  year         = {2012},
  howpublished = {Coursera Lecture: Neural Networks for Machine Learning},
  url          = {https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf}
}
@article{HORNIK1989359,
  title    = {Multilayer feedforward networks are universal approximators},
  journal  = {Neural Networks},
  volume   = {2},
  number   = {5},
  pages    = {359-366},
  year     = {1989},
  issn     = {0893-6080},
  doi      = {https://doi.org/10.1016/0893-6080(89)90020-8},
  url      = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
  author   = {Kurt Hornik and Maxwell Stinchcombe and Halbert White},
  keywords = {Feedforward networks, Universal approximation, Mapping networks, Network representation capability, Stone-Weierstrass Theorem, Squashing functions, Sigma-Pi networks, Back-propagation networks},
  abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.}
}
@misc{ioffe2015batchnormalizationacceleratingdeep,
  title         = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  author        = {Sergey Ioffe and Christian Szegedy},
  year          = {2015},
  eprint        = {1502.03167},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/1502.03167}
}
@misc{kingma2017adammethodstochasticoptimization,
  title         = {Adam: A Method for Stochastic Optimization},
  author        = {Diederik P. Kingma and Jimmy Ba},
  year          = {2017},
  eprint        = {1412.6980},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/1412.6980}
}
@misc{krotov2016denseassociativememorypattern,
  title         = {Dense Associative Memory for Pattern Recognition},
  author        = {Dmitry Krotov and John J Hopfield},
  year          = {2016},
  eprint        = {1606.01164},
  archiveprefix = {arXiv},
  primaryclass  = {cs.NE},
  url           = {https://arxiv.org/abs/1606.01164}
}
@misc{neelakantan2015addinggradientnoiseimproves,
  title         = {Adding Gradient Noise Improves Learning for Very Deep Networks},
  author        = {Arvind Neelakantan and Luke Vilnis and Quoc V. Le and Ilya Sutskever and Lukasz Kaiser and Karol Kurach and James Martens},
  year          = {2015},
  eprint        = {1511.06807},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML},
  url           = {https://arxiv.org/abs/1511.06807}
}
@article{nesterov1983method,
  title   = {A method of solving a convex programming problem with convergence rate \( O(1/k^2) \)},
  author  = {Nesterov, Yurii},
  journal = {Soviet Mathematics Doklady},
  volume  = {27},
  number  = {2},
  pages   = {372--376},
  year    = {1983}
}
@misc{ng2011mlcourse,
  author       = {Andrew Ng},
  title        = {Machine Learning},
  howpublished = {\url{https://www.coursera.org/learn/machine-learning}},
  year         = {2011},
  note         = {Coursera, Stanford University}
}
@misc{olah2015understanding,
  author       = {Christopher Olah},
  title        = {Understanding LSTM Networks},
  year         = {2015},
  howpublished = {\url{https://colah.github.io/posts/2015-08-Understanding-LSTMs/}},
  note         = {Accessed: 2025-08-02}
}
@misc{pagnoni2024bytelatenttransformerpatches,
  title         = {Byte Latent Transformer: Patches Scale Better Than Tokens},
  author        = {Artidoro Pagnoni and Ram Pasunuru and Pedro Rodriguez and John Nguyen and Benjamin Muller and Margaret Li and Chunting Zhou and Lili Yu and Jason Weston and Luke Zettlemoyer and Gargi Ghosh and Mike Lewis and Ari Holtzman and Srinivasan Iyer},
  year          = {2024},
  eprint        = {2412.09871},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2412.09871}
}
@book{prince2023understanding,
  author    = {Simon J. D. Prince},
  title     = {Understanding Deep Learning},
  publisher = {MIT Press},
  year      = {2023},
  isbn      = {9780262048644},
  url       = {https://udlbook.github.io/udlbook/}
}
@misc{ramsauer2021hopfieldnetworksneed,
  title         = {Hopfield Networks is All You Need},
  author        = {Hubert Ramsauer and Bernhard Schäfl and Johannes Lehner and Philipp Seidl and Michael Widrich and Thomas Adler and Lukas Gruber and Markus Holzleitner and Milena Pavlović and Geir Kjetil Sandve and Victor Greiff and David Kreil and Michael Kopp and Günter Klambauer and Johannes Brandstetter and Sepp Hochreiter},
  year          = {2021},
  eprint        = {2008.02217},
  archiveprefix = {arXiv},
  primaryclass  = {cs.NE},
  url           = {https://arxiv.org/abs/2008.02217},
  note          = {Blogpost: \url{https://ml-jku.github.io/hopfield-layers/}}
}
@article{SIETSMA199167,
  title    = {Creating artificial neural networks that generalize},
  journal  = {Neural Networks},
  volume   = {4},
  number   = {1},
  pages    = {67-79},
  year     = {1991},
  issn     = {0893-6080},
  doi      = {https://doi.org/10.1016/0893-6080(91)90033-2},
  url      = {https://www.sciencedirect.com/science/article/pii/0893608091900332},
  author   = {Jocelyn Sietsma and Robert J.F. Dow},
  keywords = {Neural Networks, Back-propagation, Pattern recognition, Generalization, Hidden units, Pruning},
  abstract = {We develop a technique to test the hypothesis that multilayered, feed-forward networks with few units on the first hidden layer generalize better than networks with many units in the first layer. Large networks are trained to perform a classification task and the redundant units are removed (“pruning”) to produce the smallest network capable of performing the task. A technique for inserting layers where pruning has introduced linear inseparability is also described. Two tests of ability to generalize are used—the ability to classify training inputs corrupted by noise and the ability to classify new patterns from each class. The hypothesis is found to be false for networks trained with noisy inputs. Pruning to the minimum number of units in the first layer produces networks which correctly classify the training set but generalize poorly compared with larger networks.}
}
@article{smith2017dontdecaythelearningrate,
  author     = {Samuel L. Smith and Pieter{-}Jan Kindermans and Quoc V. Le},
  title      = {Don't {D}ecay the {L}earning {R}ate, {I}ncrease the {B}atch {S}ize},
  journal    = {CoRR},
  volume     = {abs/1711.00489},
  year       = {2017},
  url        = {http://arxiv.org/abs/1711.00489},
  eprinttype = {arXiv},
  eprint     = {1711.00489},
  timestamp  = {Mon, 13 Aug 2018 16:46:33 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1711-00489.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{TangE10,
  author    = {Yichuan Tang and Chris Eliasmith},
  title     = {Deep Networks for Robust Visual Recognition},
  booktitle = {Proceedings of the 27th International Conference on Machine Learning (ICML 2010)},
  pages     = {1055--1062},
  year      = {2010},
  address   = {Haifa, Israel},
  month     = jun
}
@misc{telgarsky2016benefitsdepthneuralnetworks,
  title         = {Benefits of depth in neural networks},
  author        = {Matus Telgarsky},
  year          = {2016},
  eprint        = {1602.04485},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/1602.04485}
}
@misc{tuningplaybookgithub,
  author = {Varun Godbole and George E. Dahl and Justin Gilmer and Christopher J. Shallue and Zachary Nado},
  title  = {Deep Learning Tuning Playbook},
  url    = {http://github.com/google-research/tuning_playbook},
  year   = {2023},
  note   = {Version 1.0}
}
@misc{vaswani2023attentionneed,
  title         = {Attention Is All You Need},
  author        = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  year          = {2023},
  eprint        = {1706.03762},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/1706.03762}
}
@misc{wikipedia2025empiricalriskminimization,
  author = {{Wikipedia contributors}},
  title  = {{Empirical risk minimization} -- {Wikipedia}{,} The Free Encyclopedia},
  url    = {https://en.wikipedia.org/w/index.php?title=Empirical_risk_minimization},
  year   = {2024},
  note   = {Accessed: 2025-02-20}
}
@book{ye2022geometry,
  title     = {Geometry of Deep Learning: A Signal Processing Perspective},
  author    = {Ye, Jong Chul},
  series    = {Mathematics in Industry},
  volume    = {37},
  publisher = {Springer Singapore},
  year      = {2022},
  isbn      = {978-981-16-6045-0},
  doi       = {10.1007/978-981-16-6046-7}
}
@misc{you2019doeslearningratedecay,
  title         = {How Does Learning Rate Decay Help Modern Neural Networks?},
  author        = {Kaichao You and Mingsheng Long and Jianmin Wang and Michael I. Jordan},
  year          = {2019},
  eprint        = {1908.01878},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/1908.01878}
}
@misc{zeiler2012adadeltaadaptivelearningrate,
  title         = {ADADELTA: An Adaptive Learning Rate Method},
  author        = {Matthew D. Zeiler},
  year          = {2012},
  eprint        = {1212.5701},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/1212.5701}
}
@book{zhang2023d2l,
  title     = {Dive into Deep Learning},
  author    = {Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J.},
  publisher = {Cambridge University Press},
  note      = {\url{https://D2L.ai}},
  year      = {2023}
}
